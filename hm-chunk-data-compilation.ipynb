{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport tqdm\nimport pickle\nimport numpy as np\nimport pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chunk_path_list = ['../input/hm-trained-models/hybrid_generic/top_200_articles_chunk_1.pkl',\n                   '../input/hm-trained-models/hybrid_generic/top_200_articles_chunk_2.pkl',\n                   '../input/hm-trained-models/hybrid_generic/top_200_articles_chunk_3.pkl',\n                   '../input/hm-trained-models/hybrid_generic/top_200_articles_chunk_4.pkl']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, int_cast=True, obj_to_category=False, subset=None):\n    \"\"\"\n    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.\n    :param df: dataframe to reduce (pd.DataFrame)\n    :param int_cast: indicate if columns should be tried to be casted to int (bool)\n    :param obj_to_category: convert non-datetime related objects to category dtype (bool)\n    :param subset: subset of columns to analyse (list)\n    :return: dataset with the column dtypes adjusted (pd.DataFrame)\n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024 ** 2;\n    gc.collect()\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    cols = subset if subset is not None else df.columns.tolist()\n\n    for col in tqdm.tqdm(cols):\n        col_type = df[col].dtype\n\n        if col_type != object and col_type.name != 'category' and 'datetime' not in col_type.name:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            # test if column can be converted to an integer\n            treat_as_int = str(col_type)[:3] == 'int'\n            if int_cast and not treat_as_int:\n                treat_as_int = check_if_integer(df[col])\n\n            if treat_as_int:\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.uint8).min and c_max < np.iinfo(np.uint8).max:\n                    df[col] = df[col].astype(np.uint8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.uint16).min and c_max < np.iinfo(np.uint16).max:\n                    df[col] = df[col].astype(np.uint16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.uint32).min and c_max < np.iinfo(np.uint32).max:\n                    df[col] = df[col].astype(np.uint32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n                elif c_min > np.iinfo(np.uint64).min and c_max < np.iinfo(np.uint64).max:\n                    df[col] = df[col].astype(np.uint64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        elif 'datetime' not in col_type.name and obj_to_category:\n            df[col] = df[col].astype('category')\n    gc.collect()\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    print('Memory usage after optimization is: {:.3f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataCompilation:\n    def __init__(self, chunk_path=None):\n        self._chunk_path = chunk_path\n        self._chunk_dict = self._read_data(chunk_path)\n        \n    def _read_data(self, path):\n        with open(path, 'rb') as input_file:\n            chunk_data = pickle.load(input_file)\n            return chunk_data\n    \n    def _write_data(self, df_data):\n        df_data.to_csv(os.path.basename(self._chunk_path)+'.csv', index=False)\n        \n    def perform_stack(self, df, stacked_column_name=None):\n        stacked_df = pd.DataFrame(df[stacked_column_name].str.split(' ').tolist(), index=df.customer_id).stack()\n        stacked_df = stacked_df.reset_index()\n        stacked_df.rename(columns={0:stacked_column_name}, inplace=True)\n        return stacked_df\n\n    def transform_df(self):\n        df_final= pd.DataFrame()\n        df_final = dd.from_pandas(df_final, npartitions=4)\n\n        # Generate pandas dataframe\n        df = pd.DataFrame(self._chunk_dict.items(), columns=['customer_id', 'customer_attrs'])\n\n        # Create 2 new columns\n        df['top_200_articles'] = df['customer_attrs'].apply(lambda x: x['top_200_articles'])\n        df['top_200_scores'] = df['customer_attrs'].apply(lambda x: x['top_200_scores'])\n\n        # Drop extra column\n        df.drop(['customer_attrs'], axis = 1, inplace = True)\n\n        # article stack\n        df_article = self.perform_stack(df, 'top_200_articles')\n        df_article = reduce_mem_usage(df_article)\n        print(df_article.shape)\n\n        # score stack\n        df_score = self.perform_stack(df, 'top_200_scores')\n        df_score = reduce_mem_usage(df_score)\n        print(df_score.shape)\n\n        # Merge both stacked dataframes\n        df_final = pd.merge(df_article[['customer_id','top_200_articles']], df_score[['customer_id','top_200_scores', 'level_1']], how='left', on='customer_id')\n        df_final = reduce_mem_usage(df_final)\n        print(df_final.shape)\n        \n        del df_article\n        del df_score\n        gc.collect()\n        \n        return df_final","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for chunk_path in tqdm.tqdm(chunk_path_list, desc='Iterating through chunks'):\n    dc_instance = DataCompilation(chunk_path)\n    chunk_df = dc_instance.transform_df()\n    print(chunk_df.shape)\n    dc_instance._write_data(chunk_df)\n    del chunk_df\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}