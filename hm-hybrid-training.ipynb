{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## This experiment is to apply multiple algorithms and attain the best possible MAP@12 for the problem in hand.\n\n### Approach 1\n\n**Algorithm 1:** Find the most purchased article for each user, he/she is mostly will spent similar bucks in the same item or in some other article which is similar to that.  \nThis notebook will help us to get this algorithm up and running for each user. I will try to get the top 12 articles which are mostly assumed to be purchased by each user. If any user has purchased less than 12 articles, we will use top selling articles to complete article list till it reaches 12 in count. \n\nI will dump the submission csv for the entire dataset using this simple algorithm.\n\n**Algorithm 2:** Use LightFM algorithm to find the relation among customer and articles which have seen no interaction in past however the customer may have a tendency to buy these unpurchased articles in future visits. This notebook is already trained and hyperparameters are also optimized based on reduced transaction dates. Store top 200 predictions for each user. Please find the link to these notebooks below:  \n\nLink to Training Light FM model: https://www.kaggle.com/rickykonwar/h-m-lightfm-nofeatures  \nLink to Hyper Parameter Tuning Light FM Model: https://www.kaggle.com/rickykonwar/h-m-lightfm-nofeatures-hyperparamter-tuning/notebook\n\n### Approach 2  \n\n","metadata":{}},{"cell_type":"markdown","source":"## Importing Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport copy\nimport time\nimport tqdm\nimport pickle\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport scipy.sparse as sparse\n\nfrom multiprocessing import Pool\nfrom IPython.display import FileLink\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utility Functions","metadata":{}},{"cell_type":"code","source":"def apk(actual, predicted, k=12):\n    '''\n    Function to get Average Precision at K\n    '''\n    if len(predicted)>k:\n        predicted = predicted[:k]\n\n    score = 0.0\n    num_hits = 0.0\n\n    for i,p in enumerate(predicted):\n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits / (i+1.0)\n\n    if not actual:\n        return 0.0\n    \n    return score / min(len(actual), k)\n\ndef mapk(actual, predicted, k=12):\n    '''\n    Function to get Mean Average Precision at K\n    '''\n    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])\n\ndef create_chunk_indices(meta_df, chunk_idx, chunk_size):\n    '''\n    Function to generate chunks of data for multiprocessing\n    '''\n    start_idx = chunk_idx * chunk_size\n    end_idx = start_idx + chunk_size\n    meta_chunk = meta_df[start_idx:end_idx]\n    print(\"start/end \"+str(chunk_idx+1)+\":\" + str(start_idx) + \",\" + str(end_idx))\n    print(len(meta_chunk))\n    #chunk_idx in return value is used to sort the processed chunks back into original order,\n    return (meta_chunk, chunk_idx)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Loading","metadata":{}},{"cell_type":"code","source":"data_path = r'../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv'\ncustomer_data_path = r'../input/h-and-m-personalized-fashion-recommendations/customers.csv'\narticle_data_path = r'../input/h-and-m-personalized-fashion-recommendations/articles.csv'\nsubmission_data_path = r'../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Extraction\ndef create_data(datapath, data_type=None):\n    if data_type is None:\n        df = pd.read_csv(datapath)\n    elif data_type == 'transaction':\n        df = pd.read_csv(datapath, dtype={'article_id': str}, parse_dates=['t_dat'])\n    elif data_type == 'article':\n        df = pd.read_csv(datapath, dtype={'article_id': str})\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Load all sales data (for 3 years starting from 2018 to 2020)\n# ALso, article_id is treated as a string column otherwise it \n# would drop the leading zeros while reading the specific column values\ntransactions_data=create_data(data_path, data_type='transaction')\nprint(transactions_data.shape)\n\n# # Unique Attributes\nprint(str(len(transactions_data['t_dat'].drop_duplicates())) + \"-total No of unique transactions dates in data sheet\")\nprint(str(len(transactions_data['customer_id'].drop_duplicates())) + \"-total No of unique customers ids in data sheet\")\nprint(str(len(transactions_data['article_id'].drop_duplicates())) + \"-total No of unique article ids courses names in data sheet\")\nprint(str(len(transactions_data['sales_channel_id'].drop_duplicates())) + \"-total No of unique sales channels in data sheet\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Load all Customers\ncustomer_data=create_data(customer_data_path)\nprint(customer_data.shape)\n\nprint(str(len(customer_data['customer_id'].drop_duplicates())) + \"-total No of unique customers ids in customer data sheet\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Load all Articles\narticle_data=create_data(article_data_path, data_type='article')\nprint(article_data.shape)\n\nprint(str(len(article_data['article_id'].drop_duplicates())) + \"-total No of unique article ids in article data sheet\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Load all submission samples data\nsubmission_data=create_data(submission_data_path)\nprint(submission_data.shape)\n\nprint(str(len(submission_data['customer_id'].drop_duplicates())) + \"-total No of unique customer ids in submission data sheet\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Capturing Seasonal Effect by Limiting the transaction date\nBased on notebook with link: https://www.kaggle.com/tomooinubushi/folk-of-time-is-our-best-friend/notebook","metadata":{}},{"cell_type":"code","source":"transactions_data = transactions_data[transactions_data['t_dat'] > '2020-08-21']\ntransactions_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting transaction data to training and validation set","metadata":{}},{"cell_type":"code","source":"train_start_date = transactions_data.t_dat.min()\nsplit_date = transactions_data.t_dat.max() - datetime.timedelta(days = 7)\ntrain_transaction_data = transactions_data[(transactions_data.t_dat <= split_date) & (transactions_data.t_dat >= train_start_date)].copy()\ntest_transaction_data = transactions_data[transactions_data.t_dat > split_date].copy()\n\nprint(train_transaction_data.shape)\nprint(test_transaction_data.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transaction_data = train_transaction_data.groupby(['customer_id','article_id']).agg({'t_dat':'count'}).reset_index()\ntest_transaction_data = test_transaction_data.groupby(['customer_id','article_id']).agg({'t_dat':'count'}).reset_index()\n\ntrain_transaction_data.rename({'t_dat': 't_count'}, axis=1, inplace=True)\ntest_transaction_data.rename({'t_dat': 't_count'}, axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Approach 1\n\n## Algorithm 1: Extracting Topmost articles for each user based on number of purchase made","metadata":{}},{"cell_type":"code","source":"train_val_merge_transaction_data = train_transaction_data[train_transaction_data.article_id.isin(test_transaction_data.article_id.unique())] \ntrain_top_articles = train_val_merge_transaction_data.sort_values(['customer_id', 't_count'], ascending=False).groupby(['customer_id']).head(12)\ntest_top_articles = test_transaction_data.sort_values(['customer_id', 't_count'], ascending=False).groupby(['customer_id']).head(12)\n\n# Overall highly sold articles\noverall_top_articles = train_top_articles.groupby(['article_id'], as_index = False)['t_count'].sum().sort_values(['t_count'])['article_id'][-12:].values\noverall_top_articles = overall_top_articles[::-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_top_articles.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_top_articles.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"overall_top_articles","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluating this algorithm","metadata":{}},{"cell_type":"code","source":"%%time\n\ndef perform_cross_validation(): \n    preds, trues = [],[]\n    for customer in tqdm.tqdm(train_top_articles.customer_id.unique(), desc='Evaluating Simple Algorithm'):\n\n        predict_n_articles = train_top_articles[train_top_articles.customer_id.isin([customer])]['article_id'].values[:12]\n        actual_n_articles = test_top_articles[test_top_articles.customer_id.isin([customer])]['article_id'].values[:12]\n\n        if len(predict_n_articles) < 12:\n            predict_n_articles = list(predict_n_articles[:len(predict_n_articles)]) + list(overall_top_articles[:12 - len(predict_n_articles)])\n\n        preds.append(list(predict_n_articles))\n        trues.append(list(actual_n_articles))\n    return preds, trues","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\ntrues = []\n\nif os.path.exists(r'../input/hm-trained-models/hybrid_generic/hybrid_simplealgo_actuals.pkl') and os.path.exists(r'../input/hm-trained-models/hybrid_generic/hybrid_simplealgo_preds.pkl'):\n\n    with open(r'../input/hm-trained-models/hybrid_generic/hybrid_simplealgo_actuals.pkl', 'rb') as input_file:\n        trues = pickle.load(input_file)\n    with open(r'../input/hm-trained-models/hybrid_generic/hybrid_simplealgo_preds.pkl', 'rb') as input_file:\n        preds = pickle.load(input_file)\nelse:\n    preds, trues = perform_cross_validation()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Screenshot for predictions and actuals extraction\n\n![image.png](attachment:90fc0fb6-09b2-4e46-a1dc-bb7324ed4feb.png)","metadata":{},"attachments":{"90fc0fb6-09b2-4e46-a1dc-bb7324ed4feb.png":{"image/png":"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"}}},{"cell_type":"code","source":"# Save Predictions and Actual\nwith open('hybrid_simplealgo_preds.pkl', 'wb') as f:\n    pickle.dump(preds, f)\nwith open('hybrid_simplealgo_actuals.pkl', 'wb') as f:\n    pickle.dump(trues, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = np.round(mapk(trues, preds, k = 12), 5)\nprint(f'MAP@{12} = {score}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Getting Submission data from entire transaction data","metadata":{}},{"cell_type":"code","source":"purchase_dict = {}\n\nfor i,x in enumerate(zip(transactions_data['customer_id'], transactions_data['article_id'])):\n    cust_id, art_id = x\n    if cust_id not in purchase_dict:\n        purchase_dict[cust_id] = {}\n    \n    if art_id not in purchase_dict[cust_id]:\n        purchase_dict[cust_id][art_id] = 0\n    \n    purchase_dict[cust_id][art_id] += 1\n    \nprint(len(purchase_dict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_submission_data = submission_data[['customer_id']].copy()\nprediction_list = []\ntop_articles_list = list((transactions_data['article_id'].value_counts()).index)[:12]\ntop_prediction_string = ' '.join(top_articles_list)\n\nfor i, cust_id in enumerate(submission_data['customer_id'].values.reshape((-1,))):\n    if cust_id in purchase_dict:\n        l = sorted((purchase_dict[cust_id]).items(), key=lambda x: x[1], reverse=True)\n        l = [y[0] for y in l]\n        if len(l)>12:\n            s = ' '.join(l[:12])\n        else:\n            s = ' '.join(l+top_articles_list[:(12-len(l))])\n    else:\n        s = top_prediction_string\n    prediction_list.append(s)\n\nfinal_submission_data['prediction'] = prediction_list\nprint(final_submission_data.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_submission_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Writing Algo 1 Submissions","metadata":{}},{"cell_type":"code","source":"final_submission_data.to_csv(\"hybrid_algo1_submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FileLink(r'hybrid_algo1_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Algorithm 2: Load already trained lightfm model and predict top 200 articles for each user in submission data","metadata":{}},{"cell_type":"code","source":"lightfm_model_path = r'../input/hm-trained-models/lightfm_nofeatures/model_without_feature_optimized_01.pickle'\nlightfm_model = pickle.load(open(lightfm_model_path, 'rb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class recommendation_sampling():\n    def __init__(self, model, items = None, user_to_product_interaction_matrix = None, \n                item_features = None, user2index_map = None):\n        \n        self.user_to_product_interaction_matrix = user_to_product_interaction_matrix\n        self.item_features = item_features if item_features is not None else None\n        self.model = model\n        self.items = items\n        self.user2index_map = user2index_map\n        \n    def get_batched_recommendation(self, user, k=3, prediction_type='normal'):\n        # Getting user_indexes \n        user_index = self.user2index_map.get(user, None)\n        if user_index is None:\n            return None\n        \n        # Scores from model\n        scores = self.model.predict(user_ids = user_index, item_ids = np.arange(self.user_to_product_interaction_matrix.shape[1])) if prediction_type == 'normal' else \\\n            self.model.predict(user_ids = user_index, item_ids = np.arange(self.user_to_product_interaction_matrix.shape[1]), item_features = self.item_features)\n    \n        # Top items\n        top_items = self.items[np.argsort(-scores)][:k]\n        top_scores = np.sort(-scores)[::-1][:k]\n        \n        return top_items, top_scores\n    \n    def get_batched_recommendation_df(self, user, k=3, prediction_type='normal'):\n        # Getting user_indexes \n        user_index = self.user2index_map.get(user, None)\n        if user_index is None:\n            return None\n        \n        # Scores from model\n        scores = self.model.predict(user_ids = user_index, item_ids = np.arange(self.user_to_product_interaction_matrix.shape[1])) if prediction_type == 'normal' else \\\n            self.model.predict(user_ids = user_index, item_ids = np.arange(self.user_to_product_interaction_matrix.shape[1]), item_features = self.item_features)\n    \n        return scores\n\ndef get_customers_list():\n    # Creating a list of users\n    # return np.sort(transactions_data['customer_id'].unique()) TEMP_COMMENT\n    return np.sort(customer_data['customer_id'].unique())\n\ndef get_articles_list():\n    # Creating a list of courses \n    # item_list = transactions_data['article_id'].unique() TEMP_COMMENT\n    item_list = article_data['article_id'].unique()\n    return item_list\n\ndef id_mappings(customers_list, articles_list):\n    \"\"\"\n    \n    Create id mappings to convert user_id, item_id, and feature_id\n    \n    \"\"\"\n    customer_to_index_mapping = {}\n    index_to_customer_mapping = {}\n    for customer_index, customer_id in enumerate(customers_list):\n        customer_to_index_mapping[customer_id] = customer_index\n        index_to_customer_mapping[customer_index] = customer_id\n        \n    article_to_index_mapping = {}\n    index_to_article_mapping = {}\n    for article_index, article_id in enumerate(articles_list):\n        article_to_index_mapping[article_id] = article_index\n        index_to_article_mapping[article_index] = article_id\n        \n    return customer_to_index_mapping, index_to_customer_mapping, \\\n           article_to_index_mapping, index_to_article_mapping\n\ndef get_customer_article_interaction(customer_article_amt_df):\n    #start indexing\n    customer_article_amt_df[\"customer_id\"] = customer_article_amt_df[\"customer_id\"]\n    customer_article_amt_df[\"article_id\"] = customer_article_amt_df[\"article_id\"]\n    customer_article_amt_df[\"price\"] = customer_article_amt_df[\"price\"]\n\n    # Preprocessing dataframe created\n    customer_article_amt_df = customer_article_amt_df.rename(columns = {\"price\":\"total_amount_spent\"})\n\n    # Replace Amount Column with category codes \n    customer_article_amt_df['total_amount_spent'] = customer_article_amt_df['total_amount_spent'].astype('category')\n    customer_article_amt_df['total_amount_spent'] = customer_article_amt_df['total_amount_spent'].cat.codes\n\n    return customer_article_amt_df\n\ndef get_interaction_matrix(df, df_column_as_row, df_column_as_col, \n                        df_column_as_value, row_indexing_map, col_indexing_map):\n    \n    row = df[df_column_as_row].apply(lambda x: row_indexing_map[x]).values\n    col = df[df_column_as_col].apply(lambda x: col_indexing_map[x]).values\n    value = df[df_column_as_value].values\n    \n    return sparse.coo_matrix((value, (row, col)), shape = (len(row_indexing_map), len(col_indexing_map)))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customers = get_customers_list()\narticles = get_articles_list()\ncustomer_to_index_mapping, index_to_customer_mapping, \\\narticle_to_index_mapping, index_to_article_mapping = id_mappings(customers, articles)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transactions_data_grouped = transactions_data.groupby(['customer_id','article_id']).agg({'price':'sum','t_dat':'count'}).reset_index()\ntransactions_data_grouped = transactions_data_grouped[['customer_id','article_id','price']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_to_article = get_customer_article_interaction(customer_article_amt_df = transactions_data_grouped[['customer_id','article_id','price']])\ncustomer_to_article_interaction = get_interaction_matrix(customer_to_article, \"customer_id\", \"article_id\", \"total_amount_spent\", \\\n                                                        customer_to_index_mapping, article_to_index_mapping)            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_to_article.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_to_article_interaction","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sampling Recommendations\nrecom_without_feature = recommendation_sampling(model = lightfm_model,\n                                               items = articles,\n                                               user_to_product_interaction_matrix = customer_to_article_interaction,\n                                               user2index_map = customer_to_index_mapping)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Light FM Important Params\nn_top = 200\nlightfm_predictions_dict = {}\nlightfm_predictions_df = pd.DataFrame()\npredictions_with_df = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_top_articles(chunk):\n    lightfm_predictions_dict = {}\n    \n    for row in tqdm.tqdm(chunk[0].values, desc='Predicting top 200 articles for each user'):\n        customer = row[0]\n        top_articles, top_scores = recom_without_feature.get_batched_recommendation(user=customer, k=200)\n        lightfm_predictions_dict[customer] = {'top_200_articles': ' '.join(map(str, top_articles)),\n                                              'top_200_scores': ' '.join(map(str, top_scores))} \n       \n    return lightfm_predictions_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_cores=4\n\ndef predict_top_articles_df(chunk_submission_data=None):\n    lightfm_predictions_df = pd.DataFrame()\n    \n    for customer in tqdm.tqdm(chunk_submission_data['customer_id'].unique(), desc='Predicting top 200 articles for each user'):\n\n        articles_df = pd.DataFrame(articles, columns = ['article_id'])\n        articles_df['customer_id'] = customer\n        articles_df['lightfm_score'] = recom_without_feature.get_batched_recommendation_df(customer, len(articles))\n        articles_df.sort_values('lightfm_score', ascending = False, inplace = True)\n        articles_df['lightfm_rank'] = np.arange(articles_df.shape[0])\n\n        lightfm_predictions_df = lightfm_predictions_df.append(articles_df.head(n_top))\n        \n    return lightfm_predictions_df\n\ndef predict_top_200_articles(submission_data=None):\n    #splitting here by measurement id's to get all signals for a measurement into single chunk\n    customer_ids = submission_data[\"customer_id\"].unique()\n    df_split = np.array_split(customer_ids, num_cores)\n    chunk_size = len(df_split[0])\n    \n    chunk1 = create_chunk_indices(submission_data, 0, chunk_size)\n    chunk2 = create_chunk_indices(submission_data, 1, chunk_size)\n    chunk3 = create_chunk_indices(submission_data, 2, chunk_size)\n    chunk4 = create_chunk_indices(submission_data, 3, chunk_size)\n    \n    #list of items for multiprocessing, 4 since using 4 cores\n    all_chunks = [chunk1, chunk2, chunk3, chunk4]\n    \n    pool = Pool(num_cores)\n    result = pool.map(predict_top_articles, all_chunks)\n    \n    final_result_dict = copy.deepcopy(result[0])\n    \n    for item in result[1:]:\n        final_result_dict.update(item)\n    \n    return final_result_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy import save\n\ncustomer_ids = submission_data[\"customer_id\"].unique()\ndf_split = np.array_split(customer_ids, num_cores)\nchunk_size = len(df_split[0])\n\nchunk1 = create_chunk_indices(submission_data, 0, chunk_size)\nchunk2 = create_chunk_indices(submission_data, 1, chunk_size)\nchunk3 = create_chunk_indices(submission_data, 2, chunk_size)\nchunk4 = create_chunk_indices(submission_data, 3, chunk_size)\n\n# Getting top 200 articles for each user\nif predictions_with_df:\n    if os.path.exists(r'../input/hm-trained-models/hybrid_generic/top_200_articles_chunk_1.csv'):\n        chunk1_top_200_article_data = pd.read_csv(r'../input/hm-trained-models/hybrid_generic/top_200_articles_chunk_1.csv')\n    else:\n        print('Inferencing for first chunk started')\n        chunk1_top_200_article_data = predict_top_articles_df(chunk_submission_data = chunk1[0])\n        chunk1_top_200_article_data.to_csv('top_200_articles_chunk_1.csv', index=False)\n        FileLink(r'top_200_articles_chunk_1.csv')\n        print('Inferencing for first chunk completed')\n        \n    if os.path.exists(r'../input/hm-trained-models/hybrid_generic/top_200_articles_chunk_2.csv'):\n        chunk2_top_200_article_data = pd.read_csv(r'../input/hm-trained-models/hybrid_generic/top_200_articles_chunk_2.csv')\n    else:\n        print('Inferencing for second chunk started')\n        chunk2_top_200_article_data = predict_top_articles_df(chunk_submission_data = chunk2[0])\n        chunk2_top_200_article_data.to_csv('top_200_articles_chunk_2.csv', index=False)\n        FileLink(r'top_200_articles_chunk_2.csv')\n        print('Inferencing for second chunk completed')\n        \n    if os.path.exists(r'../input/hm-trained-models/hybrid_generic/top_200_articles_chunk_3.csv'):\n        chunk3_top_200_article_data = pd.read_csv(r'../input/hm-trained-models/hybrid_generic/top_200_articles_chunk_3.csv')\n    else:\n        print('Inferencing for third chunk started')\n        chunk3_top_200_article_data = predict_top_articles_df(chunk_submission_data = chunk3[0])\n        chunk3_top_200_article_data.to_csv('top_200_articles_chunk_3.csv', index=False)\n        FileLink(r'top_200_articles_chunk_3.csv')\n        print('Inferencing for third chunk completed')\n        \n    if os.path.exists(r'../input/hm-trained-models/hybrid_generic/top_200_articles_chunk_4.csv'):\n        chunk4_top_200_article_data = pd.read_csv(r'../input/hm-trained-models/hybrid_generic/top_200_articles_chunk_4.csv')\n    else:\n        print('Inferencing for fourth chunk started')\n        chunk4_top_200_article_data = predict_top_articles_df(chunk_submission_data = chunk4[0])\n        chunk4_top_200_article_data.to_csv('top_200_articles_chunk_4.csv', index=False)\n        FileLink(r'top_200_articles_chunk_4.csv')\n        print('Inferencing for fourth chunk completed')\n    \n    # top_200_articles_path = r'../input/hm-trained-models/hybrid_generic/top_200_articles.csv'\n    # lightfm_predictions_df = pd.read_csv(top_200_articles_path) if os.path.exists(top_200_articles_path) else predict_top_articles_df()\nelse:\n    if os.path.exists(r'../input/hm-trained-models/hybrid_generic/top_200_articles_chunk_1.pkl'):\n        with open(r'../input/hm-trained-models/hybrid_generic/top_200_articles_chunk_1.pkl', 'rb') as input_file:\n            chunk1_top_200_article_dict = pickle.load(input_file)\n    else:\n        print('Inferencing for first chunk started')\n        chunk1_top_200_article_dict = predict_top_articles(chunk1)\n        with open('top_200_articles_chunk_1.pkl', 'wb') as f:\n            pickle.dump(chunk1_top_200_article_dict, f)\n        print('Inferencing for first chunk completed')\n        \n    if os.path.exists(r'../input/hm-trained-models/hybrid_generic/top_200_articles_chunk_2.pkl'):\n        with open(r'../input/hm-trained-models/hybrid_generic/top_200_articles_chunk_2.pkl', 'rb') as input_file:\n            chunk2_top_200_article_dict = pickle.load(input_file)\n    else:\n        print('Inferencing for second chunk started')\n        chunk2_top_200_article_dict = predict_top_articles(chunk2)\n        with open('top_200_articles_chunk_2.pkl', 'wb') as f:\n            pickle.dump(chunk2_top_200_article_dict, f)\n        print('Inferencing for second chunk completed')\n        \n    if os.path.exists(r'../input/hm-trained-models/hybrid_generic/top_200_articles_chunk_3.pkl'):\n        with open(r'../input/hm-trained-models/hybrid_generic/top_200_articles_chunk_3.pkl', 'rb') as input_file:\n            chunk3_top_200_article_dict = pickle.load(input_file)\n    else:\n        print('Inferencing for third chunk started')\n        chunk3_top_200_article_dict = predict_top_articles(chunk3)\n        with open('top_200_articles_chunk_3.pkl', 'wb') as f:\n            pickle.dump(chunk3_top_200_article_dict, f)\n        print('Inferencing for third chunk completed')\n        \n    if os.path.exists(r'../input/hm-trained-models/hybrid_generic/top_200_articles_chunk_4.pkl'):\n        with open(r'../input/hm-trained-models/hybrid_generic/top_200_articles_chunk_4.pkl', 'rb') as input_file:\n            chunk4_top_200_article_dict = pickle.load(input_file)\n    else:\n        print('Inferencing for fourth chunk started')\n        chunk4_top_200_article_dict = predict_top_articles(chunk4)\n        with open('top_200_articles_chunk_4.pkl', 'wb') as f:\n            pickle.dump(chunk4_top_200_article_dict, f)\n        print('Inferencing for fourth chunk completed')\n        \n    # top_200_articles_dict_path = r'../input/hm-trained-models/hybrid_generic/top_200_articles.npy'\n    # lightfm_predictions_dict = np.load(top_200_articles_dict_path) if os.path.exists(top_200_articles_dict_path) else predict_top_200_articles(submission_data)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if predictions_with_df:\n#     lightfm_predictions_df.to_csv('top_200_articles.csv', index=False)\n# else:\n#     from numpy import save\n#     save('./top_200_articles.npy', lightfm_predictions_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Adding Customer and Article Indices Columns\n# if predictions_with_df:\n#     lightfm_predictions_df['customer_index'] = lightfm_predictions_df['customer_id'].map(lambda x: customer_to_index_mapping.get(x))\n#     lightfm_predictions_df['article_index'] = lightfm_predictions_df['article_id'].map(lambda x: article_to_index_mapping.get(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}