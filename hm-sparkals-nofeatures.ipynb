{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This experiment was performed using cornac module which a very popular recommender module used for research based works in recommender field and is quickly gaining the popularity. Please check out their official documentation in the link mentioned below:  \n\n**Link to Cornac Official Site:** https://cornac.readthedocs.io/en/latest/  \n**Link to Cornac Doucmentation:** https://cornac.readthedocs.io/_/downloads/en/latest/pdf/ ","metadata":{}},{"cell_type":"code","source":"!pip install recommenders==1.1.0","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-12T14:17:41.107973Z","iopub.execute_input":"2022-04-12T14:17:41.108398Z","iopub.status.idle":"2022-04-12T14:18:00.535381Z","shell.execute_reply.started":"2022-04-12T14:17:41.108283Z","shell.execute_reply":"2022-04-12T14:18:00.534443Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting recommenders==1.1.0\n  Downloading recommenders-1.1.0-py3-none-manylinux1_x86_64.whl (335 kB)\n     |████████████████████████████████| 335 kB 629 kB/s            \n\u001b[?25hRequirement already satisfied: matplotlib<4,>=2.2.2 in /opt/conda/lib/python3.7/site-packages (from recommenders==1.1.0) (3.5.1)\nRequirement already satisfied: pandas<2,>1.0.3 in /opt/conda/lib/python3.7/site-packages (from recommenders==1.1.0) (1.3.5)\nRequirement already satisfied: lightgbm>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from recommenders==1.1.0) (3.3.1)\nRequirement already satisfied: scikit-learn<1.0.3,>=0.22.1 in /opt/conda/lib/python3.7/site-packages (from recommenders==1.1.0) (1.0.1)\nRequirement already satisfied: bottleneck<2,>=1.2.1 in /opt/conda/lib/python3.7/site-packages (from recommenders==1.1.0) (1.3.2)\nCollecting cornac<2,>=1.1.2\n  Downloading cornac-1.14.2-cp37-cp37m-manylinux1_x86_64.whl (12.4 MB)\n     |████████████████████████████████| 12.4 MB 9.6 MB/s            \n\u001b[?25hRequirement already satisfied: transformers<5,>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from recommenders==1.1.0) (4.16.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from recommenders==1.1.0) (2.26.0)\nCollecting category-encoders<2,>=1.3.0\n  Downloading category_encoders-1.3.0-py2.py3-none-any.whl (61 kB)\n     |████████████████████████████████| 61 kB 5.9 MB/s             \n\u001b[?25hRequirement already satisfied: numpy>=1.19 in /opt/conda/lib/python3.7/site-packages (from recommenders==1.1.0) (1.20.3)\nRequirement already satisfied: lightfm<2,>=1.15 in /opt/conda/lib/python3.7/site-packages (from recommenders==1.1.0) (1.16)\nRequirement already satisfied: scipy<2,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from recommenders==1.1.0) (1.7.3)\nRequirement already satisfied: jinja2<3.1,>=2 in /opt/conda/lib/python3.7/site-packages (from recommenders==1.1.0) (3.0.3)\nRequirement already satisfied: tqdm<5,>=4.31.1 in /opt/conda/lib/python3.7/site-packages (from recommenders==1.1.0) (4.62.3)\nCollecting pandera[strategies]>=0.6.5\n  Downloading pandera-0.9.0-py3-none-any.whl (197 kB)\n     |████████████████████████████████| 197 kB 69.9 MB/s            \n\u001b[?25hCollecting pyyaml<6,>=5.4.1\n  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n     |████████████████████████████████| 636 kB 59.1 MB/s            \n\u001b[?25hRequirement already satisfied: numba<1,>=0.38.1 in /opt/conda/lib/python3.7/site-packages (from recommenders==1.1.0) (0.54.1)\nRequirement already satisfied: seaborn<1,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from recommenders==1.1.0) (0.11.2)\nRequirement already satisfied: memory-profiler<1,>=0.54.0 in /opt/conda/lib/python3.7/site-packages (from recommenders==1.1.0) (0.60.0)\nRequirement already satisfied: scikit-surprise>=1.0.6 in /opt/conda/lib/python3.7/site-packages (from recommenders==1.1.0) (1.1.1)\nCollecting nltk<4,>=3.4\n  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n     |████████████████████████████████| 1.5 MB 39.8 MB/s            \n\u001b[?25hRequirement already satisfied: retrying>=1.3.3 in /opt/conda/lib/python3.7/site-packages (from recommenders==1.1.0) (1.3.3)\nRequirement already satisfied: statsmodels>=0.6.1 in /opt/conda/lib/python3.7/site-packages (from category-encoders<2,>=1.3.0->recommenders==1.1.0) (0.13.1)\nRequirement already satisfied: patsy>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from category-encoders<2,>=1.3.0->recommenders==1.1.0) (0.5.2)\nCollecting powerlaw\n  Downloading powerlaw-1.5-py3-none-any.whl (24 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2<3.1,>=2->recommenders==1.1.0) (2.1.1)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from lightgbm>=2.2.1->recommenders==1.1.0) (0.37.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib<4,>=2.2.2->recommenders==1.1.0) (4.28.4)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib<4,>=2.2.2->recommenders==1.1.0) (3.0.6)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib<4,>=2.2.2->recommenders==1.1.0) (1.3.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib<4,>=2.2.2->recommenders==1.1.0) (2.8.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib<4,>=2.2.2->recommenders==1.1.0) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib<4,>=2.2.2->recommenders==1.1.0) (8.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib<4,>=2.2.2->recommenders==1.1.0) (0.11.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from memory-profiler<1,>=0.54.0->recommenders==1.1.0) (5.8.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk<4,>=3.4->recommenders==1.1.0) (8.0.3)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk<4,>=3.4->recommenders==1.1.0) (1.1.0)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk<4,>=3.4->recommenders==1.1.0) (2021.11.10)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from numba<1,>=0.38.1->recommenders==1.1.0) (59.5.0)\nRequirement already satisfied: llvmlite<0.38,>=0.37.0rc1 in /opt/conda/lib/python3.7/site-packages (from numba<1,>=0.38.1->recommenders==1.1.0) (0.37.0)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas<2,>1.0.3->recommenders==1.1.0) (2021.3)\nRequirement already satisfied: pydantic in /opt/conda/lib/python3.7/site-packages (from pandera[strategies]>=0.6.5->recommenders==1.1.0) (1.8.2)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.7/site-packages (from pandera[strategies]>=0.6.5->recommenders==1.1.0) (1.13.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from pandera[strategies]>=0.6.5->recommenders==1.1.0) (4.1.1)\nRequirement already satisfied: typing-inspect>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from pandera[strategies]>=0.6.5->recommenders==1.1.0) (0.7.1)\nRequirement already satisfied: pyarrow in /opt/conda/lib/python3.7/site-packages (from pandera[strategies]>=0.6.5->recommenders==1.1.0) (6.0.1)\nCollecting hypothesis>=5.41.1\n  Downloading hypothesis-6.43.0-py3-none-any.whl (381 kB)\n     |████████████████████████████████| 381 kB 46.3 MB/s            \n\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->recommenders==1.1.0) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->recommenders==1.1.0) (1.26.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->recommenders==1.1.0) (3.1)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->recommenders==1.1.0) (2.0.9)\nRequirement already satisfied: six>=1.7.0 in /opt/conda/lib/python3.7/site-packages (from retrying>=1.3.3->recommenders==1.1.0) (1.16.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn<1.0.3,>=0.22.1->recommenders==1.1.0) (3.0.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers<5,>=2.5.0->recommenders==1.1.0) (4.11.3)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers<5,>=2.5.0->recommenders==1.1.0) (0.4.0)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers<5,>=2.5.0->recommenders==1.1.0) (0.0.49)\nRequirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers<5,>=2.5.0->recommenders==1.1.0) (0.11.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers<5,>=2.5.0->recommenders==1.1.0) (3.4.0)\nRequirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from hypothesis>=5.41.1->pandera[strategies]>=0.6.5->recommenders==1.1.0) (2.4.0)\nRequirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.7/site-packages (from hypothesis>=5.41.1->pandera[strategies]>=0.6.5->recommenders==1.1.0) (21.2.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from typing-inspect>=0.6.0->pandera[strategies]>=0.6.5->recommenders==1.1.0) (0.4.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers<5,>=2.5.0->recommenders==1.1.0) (3.6.0)\nRequirement already satisfied: mpmath in /opt/conda/lib/python3.7/site-packages (from powerlaw->cornac<2,>=1.1.2->recommenders==1.1.0) (1.2.1)\nInstalling collected packages: pyyaml, powerlaw, pandera, hypothesis, nltk, cornac, category-encoders, recommenders\n  Attempting uninstall: pyyaml\n    Found existing installation: PyYAML 6.0\n    Uninstalling PyYAML-6.0:\n      Successfully uninstalled PyYAML-6.0\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n  Attempting uninstall: category-encoders\n    Found existing installation: category-encoders 2.4.0\n    Uninstalling category-encoders-2.4.0:\n      Successfully uninstalled category-encoders-2.4.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.7 which is incompatible.\u001b[0m\nSuccessfully installed category-encoders-1.3.0 cornac-1.14.2 hypothesis-6.43.0 nltk-3.7 pandera-0.9.0 powerlaw-1.5 pyyaml-5.4.1 recommenders-1.1.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pyspark==3.2.1","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-12T14:18:00.537482Z","iopub.execute_input":"2022-04-12T14:18:00.537784Z","iopub.status.idle":"2022-04-12T14:18:52.652338Z","shell.execute_reply.started":"2022-04-12T14:18:00.537745Z","shell.execute_reply":"2022-04-12T14:18:52.651084Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pyspark==3.2.1\n  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n     |████████████████████████████████| 281.4 MB 34 kB/s              \n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting py4j==0.10.9.3\n  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n     |████████████████████████████████| 198 kB 35.9 MB/s            \n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=38ca269bc07beb917d92ec0cdc0097395a9421d86c95e6afd48492005bca606f\n  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\n  Attempting uninstall: py4j\n    Found existing installation: py4j 0.10.9.4\n    Uninstalling py4j-0.10.9.4:\n      Successfully uninstalled py4j-0.10.9.4\nSuccessfully installed py4j-0.10.9.3 pyspark-3.2.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport sys\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nimport pyspark\nimport pyspark.sql.functions as F\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.sql.types import StructType, StructField\nfrom pyspark.sql.types import FloatType, IntegerType, LongType, StringType, TimestampType\n\nfrom recommenders.evaluation.spark_evaluation import SparkRankingEvaluation, SparkRatingEvaluation\nfrom recommenders.tuning.parameter_sweep import generate_param_grid\nfrom recommenders.datasets.spark_splitters import spark_random_split\n\nprint(\"System version: {}\".format(sys.version))\nprint(\"Pandas version: {}\".format(pd.__version__))\nprint(\"PySpark version: {}\".format(pyspark.__version__))","metadata":{"execution":{"iopub.status.busy":"2022-04-12T14:18:52.655417Z","iopub.execute_input":"2022-04-12T14:18:52.655749Z","iopub.status.idle":"2022-04-12T14:18:53.924811Z","shell.execute_reply.started":"2022-04-12T14:18:52.655708Z","shell.execute_reply":"2022-04-12T14:18:53.923815Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"System version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \n[GCC 9.4.0]\nPandas version: 1.3.5\nPySpark version: 3.2.1\n","output_type":"stream"}]},{"cell_type":"code","source":"data_path = r'../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv'\ncustomer_data_path = r'../input/h-and-m-personalized-fashion-recommendations/customers.csv'\narticle_data_path = r'../input/h-and-m-personalized-fashion-recommendations/articles.csv'\nsubmission_data_path = r'../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv'","metadata":{"execution":{"iopub.status.busy":"2022-04-12T14:18:53.927726Z","iopub.execute_input":"2022-04-12T14:18:53.928092Z","iopub.status.idle":"2022-04-12T14:18:53.934141Z","shell.execute_reply.started":"2022-04-12T14:18:53.928048Z","shell.execute_reply":"2022-04-12T14:18:53.932863Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"sc = SparkSession.builder.appName(\"H&M_ALS_Recommender\").config(\"spark.sql.files.maxPartitionBytes\", 5000000).getOrCreate()\nspark = SparkSession(sc)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T14:18:53.935470Z","iopub.execute_input":"2022-04-12T14:18:53.936043Z","iopub.status.idle":"2022-04-12T14:19:00.490962Z","shell.execute_reply.started":"2022-04-12T14:18:53.936003Z","shell.execute_reply":"2022-04-12T14:19:00.489757Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"WARNING: An illegal reflective access operation has occurred\nWARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/conda/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\nWARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\nWARNING: All illegal access operations will be denied in a future release\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n22/04/12 14:18:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}]},{"cell_type":"code","source":"transaction_sp_df = spark.read.option(\"header\",True).csv(data_path)\ntransaction_sp_df.printSchema()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T14:19:00.492598Z","iopub.execute_input":"2022-04-12T14:19:00.492991Z","iopub.status.idle":"2022-04-12T14:19:06.725866Z","shell.execute_reply.started":"2022-04-12T14:19:00.492940Z","shell.execute_reply":"2022-04-12T14:19:06.724968Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"root\n |-- t_dat: string (nullable = true)\n |-- customer_id: string (nullable = true)\n |-- article_id: string (nullable = true)\n |-- price: string (nullable = true)\n |-- sales_channel_id: string (nullable = true)\n\n","output_type":"stream"}]},{"cell_type":"code","source":"min_date, max_date = transaction_sp_df.select(F.min(\"t_dat\"), F.max(\"t_dat\")).first()\nmin_date, max_date","metadata":{"execution":{"iopub.status.busy":"2022-04-12T14:19:06.727209Z","iopub.execute_input":"2022-04-12T14:19:06.727561Z","iopub.status.idle":"2022-04-12T14:19:44.340409Z","shell.execute_reply.started":"2022-04-12T14:19:06.727518Z","shell.execute_reply":"2022-04-12T14:19:44.339466Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"('2018-09-20', '2020-09-22')"},"metadata":{}}]},{"cell_type":"code","source":"transaction_sp_df =  transaction_sp_df.withColumn('t_dat', transaction_sp_df['t_dat'].cast('string'))\ntransaction_sp_df = transaction_sp_df.withColumn('date', F.from_unixtime(F.unix_timestamp('t_dat', 'yyyy-MM-dd')))\n\ndate_to_filter = F.to_date(F.lit('2020-08-21')).cast(TimestampType())\n\ntransaction_sp_df = transaction_sp_df.filter((transaction_sp_df['date']>date_to_filter))\ntransaction_sp_df.count()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T14:19:44.346294Z","iopub.execute_input":"2022-04-12T14:19:44.348125Z","iopub.status.idle":"2022-04-12T14:20:44.248402Z","shell.execute_reply.started":"2022-04-12T14:19:44.348060Z","shell.execute_reply":"2022-04-12T14:20:44.247333Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"1190911"},"metadata":{}}]},{"cell_type":"code","source":"transaction_sp_df =  transaction_sp_df.withColumn('price', transaction_sp_df['price'].cast('float'))\ntransaction_sp_df = transaction_sp_df.groupby(['customer_id','article_id']).agg(F.sum('price').alias('purchase_amount'), \\\n                                                                                F.count('t_dat').alias('purchase_count'))\ntransaction_sp_df.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T14:20:44.250350Z","iopub.execute_input":"2022-04-12T14:20:44.256130Z","iopub.status.idle":"2022-04-12T14:21:56.402624Z","shell.execute_reply.started":"2022-04-12T14:20:44.256062Z","shell.execute_reply":"2022-04-12T14:21:56.401780Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"[Stage 7:======================================================>(697 + 1) / 698]\r","output_type":"stream"},{"name":"stdout","text":"+--------------------+----------+--------------------+--------------+\n|         customer_id|article_id|     purchase_amount|purchase_count|\n+--------------------+----------+--------------------+--------------+\n|009f4e304a83016f8...|0903276002| 0.04083050787448883|             1|\n|036e6dd5bf47b97e2...|0806388003|0.013542372733354568|             1|\n|03dd3e86d9e9b3191...|0715624001| 0.02540677972137928|             1|\n|0509509190fd57e3f...|0815004005|0.022016949951648712|             1|\n|056729b03521f2526...|0852374013|0.033881355077028275|             1|\n|05d5fd6625b0521f7...|0909924004|0.029999999329447746|             1|\n|06ccd9c0f6b4a33f4...|0817401001|0.043389830738306046|             2|\n|07752786feb0296c6...|0865932001|0.023152543231844902|             1|\n|080f4e977c1bdf5a1...|0629758005|0.005067796446382999|             1|\n|0b1d12e082618d454...|0682236001| 0.02540677972137928|             1|\n|0c5da9abf14fe5b85...|0890197001|0.033881355077028275|             1|\n|0ddd5f545d26b7d34...|0570189002| 0.02540677972137928|             1|\n|0e1478bd7b1579226...|0786304001| 0.02540677972137928|             1|\n|10fce0f079378a98a...|0915526001|0.033881355077028275|             1|\n|116a4629c26474bdc...|0598755001|0.013542372733354568|             1|\n|1185eb8a8ddf56129...|0688728023| 0.02540677972137928|             1|\n|14c0136af5cc986f3...|0805000010|0.038118645548820496|             1|\n|15aed804e191e5762...|0853612001|0.016915254294872284|             2|\n|17fed5541027e3288...|0864040001|0.022016949951648712|             1|\n|186791c28bef73f29...|0827968005| 0.01630508527159691|             1|\n+--------------------+----------+--------------------+--------------+\nonly showing top 20 rows\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"indexer = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in list(set(transaction_sp_df.columns)-set(['purchase_amount','purchase_count'])) ]\npipeline = Pipeline(stages=indexer)\ntransaction_indexed_sp_df = pipeline.fit(transaction_sp_df).transform(transaction_sp_df)\ntransaction_indexed_sp_df.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T14:21:56.406321Z","iopub.execute_input":"2022-04-12T14:21:56.406676Z","iopub.status.idle":"2022-04-12T14:25:23.516072Z","shell.execute_reply.started":"2022-04-12T14:21:56.406631Z","shell.execute_reply":"2022-04-12T14:25:23.514978Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"22/04/12 14:25:22 WARN DAGScheduler: Broadcasting large task binary with size 23.0 MiB\n[Stage 24:>                                                         (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"+--------------------+----------+--------------------+--------------+-----------------+----------------+\n|         customer_id|article_id|     purchase_amount|purchase_count|customer_id_index|article_id_index|\n+--------------------+----------+--------------------+--------------+-----------------+----------------+\n|009f4e304a83016f8...|0903276002| 0.04083050787448883|             1|          10737.0|          1832.0|\n|036e6dd5bf47b97e2...|0806388003|0.013542372733354568|             1|         142309.0|           401.0|\n|03dd3e86d9e9b3191...|0715624001| 0.02540677972137928|             1|          77338.0|            17.0|\n|0509509190fd57e3f...|0815004005|0.022016949951648712|             1|          16642.0|         13955.0|\n|056729b03521f2526...|0852374013|0.033881355077028275|             1|         104661.0|          2446.0|\n|05d5fd6625b0521f7...|0909924004|0.029999999329447746|             1|         142815.0|          1996.0|\n|06ccd9c0f6b4a33f4...|0817401001|0.043389830738306046|             2|          26837.0|          2832.0|\n|07752786feb0296c6...|0865932001|0.023152543231844902|             1|         143154.0|          1254.0|\n|080f4e977c1bdf5a1...|0629758005|0.005067796446382999|             1|           8746.0|          4070.0|\n|0b1d12e082618d454...|0682236001| 0.02540677972137928|             1|          44779.0|          1581.0|\n|0c5da9abf14fe5b85...|0890197001|0.033881355077028275|             1|          78226.0|          6584.0|\n|0ddd5f545d26b7d34...|0570189002| 0.02540677972137928|             1|          27059.0|          1497.0|\n|0e1478bd7b1579226...|0786304001| 0.02540677972137928|             1|          78435.0|           521.0|\n|10fce0f079378a98a...|0915526001|0.033881355077028275|             1|          27156.0|             3.0|\n|116a4629c26474bdc...|0598755001|0.013542372733354568|             1|         106435.0|           898.0|\n|1185eb8a8ddf56129...|0688728023| 0.02540677972137928|             1|          45145.0|           665.0|\n|14c0136af5cc986f3...|0805000010|0.038118645548820496|             1|         106953.0|           501.0|\n|15aed804e191e5762...|0853612001|0.016915254294872284|             2|          21408.0|          2838.0|\n|17fed5541027e3288...|0864040001|0.022016949951648712|             1|          79480.0|           689.0|\n|186791c28bef73f29...|0827968005| 0.01630508527159691|             1|           2771.0|          2811.0|\n+--------------------+----------+--------------------+--------------+-----------------+----------------+\nonly showing top 20 rows\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"implicit_feedback_columns = ['customer_id_index','article_id_index','purchase_count']\ntrain_sp_df, test_sp_df = spark_random_split(transaction_indexed_sp_df.select(*implicit_feedback_columns), ratio=0.75, seed=42)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T14:25:23.517750Z","iopub.execute_input":"2022-04-12T14:25:23.518149Z","iopub.status.idle":"2022-04-12T14:25:23.580422Z","shell.execute_reply.started":"2022-04-12T14:25:23.518100Z","shell.execute_reply":"2022-04-12T14:25:23.579681Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"RANK = 10\nMAX_ITER = 15\nREG_PARAM = 0.05\nK=12","metadata":{"execution":{"iopub.status.busy":"2022-04-12T14:25:23.581442Z","iopub.execute_input":"2022-04-12T14:25:23.581688Z","iopub.status.idle":"2022-04-12T14:25:23.588736Z","shell.execute_reply.started":"2022-04-12T14:25:23.581659Z","shell.execute_reply":"2022-04-12T14:25:23.587661Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"als = ALS(\n    maxIter=MAX_ITER, \n    rank=RANK,\n    regParam=REG_PARAM, \n    userCol='customer_id_index', \n    itemCol='article_id_index', \n    ratingCol='purchase_count', \n    coldStartStrategy=\"drop\",\n    nonnegative=True\n)\n\nmodel = als.fit(train_sp_df)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T14:25:23.590861Z","iopub.execute_input":"2022-04-12T14:25:23.591629Z","iopub.status.idle":"2022-04-12T14:30:11.994506Z","shell.execute_reply.started":"2022-04-12T14:25:23.591575Z","shell.execute_reply":"2022-04-12T14:30:11.993269Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"22/04/12 14:27:34 WARN DAGScheduler: Broadcasting large task binary with size 23.0 MiB\n22/04/12 14:27:36 WARN DAGScheduler: Broadcasting large task binary with size 23.0 MiB\n22/04/12 14:27:40 WARN DAGScheduler: Broadcasting large task binary with size 23.0 MiB\n22/04/12 14:27:42 WARN DAGScheduler: Broadcasting large task binary with size 23.0 MiB\n22/04/12 14:27:47 WARN DAGScheduler: Broadcasting large task binary with size 23.0 MiB\n22/04/12 14:27:49 WARN DAGScheduler: Broadcasting large task binary with size 23.0 MiB\n22/04/12 14:27:55 WARN DAGScheduler: Broadcasting large task binary with size 23.0 MiB\n22/04/12 14:27:58 WARN DAGScheduler: Broadcasting large task binary with size 23.0 MiB\n22/04/12 14:28:02 WARN DAGScheduler: Broadcasting large task binary with size 23.0 MiB\n22/04/12 14:28:07 WARN DAGScheduler: Broadcasting large task binary with size 23.0 MiB\n22/04/12 14:28:10 WARN DAGScheduler: Broadcasting large task binary with size 23.0 MiB\n22/04/12 14:28:16 WARN DAGScheduler: Broadcasting large task binary with size 23.0 MiB\n22/04/12 14:28:19 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n22/04/12 14:28:24 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n22/04/12 14:28:28 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n22/04/12 14:28:32 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n22/04/12 14:28:36 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n22/04/12 14:28:43 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n22/04/12 14:28:47 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n22/04/12 14:28:52 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n22/04/12 14:28:55 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n22/04/12 14:29:00 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n22/04/12 14:29:04 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n22/04/12 14:29:09 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n22/04/12 14:29:12 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n22/04/12 14:29:17 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n22/04/12 14:29:20 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n22/04/12 14:29:25 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n22/04/12 14:29:28 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n22/04/12 14:29:34 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n22/04/12 14:29:37 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n22/04/12 14:29:42 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n22/04/12 14:29:45 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n22/04/12 14:29:50 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n22/04/12 14:29:54 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n22/04/12 14:29:59 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n22/04/12 14:30:02 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n22/04/12 14:30:08 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"# Get the cross join of all user-item pairs and score them.\nusers = train_sp_df.select('customer_id_index').distinct()\nitems = train_sp_df.select('article_id_index').distinct()\nuser_item = users.crossJoin(items)\n\npreds_sp_df = model.transform(user_item)\n\n# Remove seen items.\npreds_sp_df_exclude_train = preds_sp_df.alias(\"pred\").join(\n    train_sp_df.alias(\"train\"),\n    (preds_sp_df['customer_id_index'] == train_sp_df['customer_id_index']) & (preds_sp_df['article_id_index'] == train_sp_df['article_id_index']),\n    how='outer'\n)\n\npreds_final_sp_df = preds_sp_df_exclude_train.filter(preds_sp_df_exclude_train[\"train.purchase_count\"].isNull()) \\\n    .select('pred.' + 'customer_id_index', 'pred.' + 'article_id_index', 'pred.' + \"prediction\")\n\npreds_final_sp_df.show(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T14:30:12.000959Z","iopub.execute_input":"2022-04-12T14:30:12.002180Z","iopub.status.idle":"2022-04-12T15:15:17.694297Z","shell.execute_reply.started":"2022-04-12T14:30:12.002110Z","shell.execute_reply":"2022-04-12T15:15:17.689731Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"22/04/12 14:30:13 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n22/04/12 14:30:14 WARN DAGScheduler: Broadcasting large task binary with size 23.1 MiB\n22/04/12 14:31:32 WARN DAGScheduler: Broadcasting large task binary with size 23.0 MiB\n22/04/12 14:31:34 WARN DAGScheduler: Broadcasting large task binary with size 23.0 MiB\n22/04/12 14:31:36 WARN DAGScheduler: Broadcasting large task binary with size 23.0 MiB\n22/04/12 14:31:44 WARN DAGScheduler: Broadcasting large task binary with size 23.0 MiB\n22/04/12 14:31:48 WARN DAGScheduler: Broadcasting large task binary with size 23.0 MiB\n22/04/12 15:14:37 WARN DAGScheduler: Broadcasting large task binary with size 23.2 MiB\n22/04/12 15:15:16 ERROR Executor: Exception in task 0.0 in stage 225.0 (TID 6686)\njava.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:778)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2460/0x0000000840ff0440.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n22/04/12 15:15:16 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 225.0 (TID 6686),5,main]\njava.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:778)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2460/0x0000000840ff0440.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n22/04/12 15:15:16 WARN TaskSetManager: Lost task 0.0 in stage 225.0 (TID 6686) (d240acd3afc5 executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:778)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2460/0x0000000840ff0440.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n22/04/12 15:15:16 ERROR TaskSetManager: Task 0 in stage 225.0 failed 1 times; aborting job\n22/04/12 15:15:16 WARN TaskSetManager: Lost task 1.0 in stage 225.0 (TID 6687) (d240acd3afc5 executor driver): TaskKilled (Stage cancelled)\n22/04/12 15:15:16 WARN TaskSetManager: Lost task 3.0 in stage 225.0 (TID 6689) (d240acd3afc5 executor driver): TaskKilled (Stage cancelled)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_51/3250844716.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pred.'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'customer_id_index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pred.'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'article_id_index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pred.'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mpreds_final_sp_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o318.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 225.0 failed 1 times, most recent failure: Lost task 0.0 in stage 225.0 (TID 6686) (d240acd3afc5 executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:778)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2460/0x0000000840ff0440.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:778)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2460/0x0000000840ff0440.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"],"ename":"Py4JJavaError","evalue":"An error occurred while calling o318.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 225.0 failed 1 times, most recent failure: Lost task 0.0 in stage 225.0 (TID 6686) (d240acd3afc5 executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:778)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2460/0x0000000840ff0440.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:523)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:778)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2460/0x0000000840ff0440.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n","output_type":"error"}]},{"cell_type":"code","source":"evaluations = SparkRankingEvaluation(\n    test_sp_df, \n    preds_final_sp_df,\n    col_user='customer_id_index',\n    col_item='article_id_index',\n    col_rating='purchase_count',\n    col_prediction='prediction',\n    k=K\n)\n\nprint(\n    \"Precision@k = {}\".format(evaluations.precision_at_k()),\n    \"Recall@k = {}\".format(evaluations.recall_at_k()),\n    \"NDCG@k = {}\".format(evaluations.ndcg_at_k()),\n    \"Mean average precision = {}\".format(evaluations.map_at_k()),\n    sep=\"\\n\"\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T15:15:17.696829Z","iopub.status.idle":"2022-04-12T15:15:17.697444Z","shell.execute_reply.started":"2022-04-12T15:15:17.697183Z","shell.execute_reply":"2022-04-12T15:15:17.697213Z"},"trusted":true},"execution_count":null,"outputs":[]}]}